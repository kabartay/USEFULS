{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "In this notebook we prepare a simple solution for the [kaggle challenge on trigger system.](https://inclass.kaggle.com/c/trigger-system)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy\n",
    "from rep.utils import train_test_split_group\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-06-20 10:31:34--  https://2016.mlhep.yandex.net/data/training.csv\n",
      "Resolving 2016.mlhep.yandex.net... 2a02:6b8::1:208, 213.180.193.208\n",
      "Connecting to 2016.mlhep.yandex.net|2a02:6b8::1:208|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 83646329 (80M) [application/octet-stream]\n",
      "Saving to: 'training.csv'\n",
      "\n",
      "training.csv        100%[===================>]  79.77M  1.26MB/s    in 1m 40s  \n",
      "\n",
      "2016-06-20 10:33:15 (816 KB/s) - 'training.csv' saved [83646329/83646329]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd datasets; wget -O training.csv -nc --no-check-certificate https://2016.mlhep.yandex.net/data/training.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-06-20 10:33:17--  https://2016.mlhep.yandex.net/data/test.csv\n",
      "Resolving 2016.mlhep.yandex.net... 2a02:6b8::1:208, 213.180.193.208\n",
      "Connecting to 2016.mlhep.yandex.net|2a02:6b8::1:208|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 68944694 (66M) [application/octet-stream]\n",
      "Saving to: 'test.csv'\n",
      "\n",
      "test.csv            100%[===================>]  65.75M  1.44MB/s    in 44s     \n",
      "\n",
      "2016-06-20 10:34:02 (1.49 MB/s) - 'test.csv' saved [68944694/68944694]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd datasets; wget -O test.csv -nc --no-check-certificate https://2016.mlhep.yandex.net/data/test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read training and test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pandas.read_csv('datasets/training.csv')\n",
    "test = pandas.read_csv('datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EventID</th>\n",
       "      <th>Label</th>\n",
       "      <th>Mass</th>\n",
       "      <th>Corrected_mass</th>\n",
       "      <th>Pt</th>\n",
       "      <th>Pt_sum</th>\n",
       "      <th>Pt_min</th>\n",
       "      <th>IP_chi2</th>\n",
       "      <th>IP_chi2_sum</th>\n",
       "      <th>Flight_distance</th>\n",
       "      <th>Pseudorapidity</th>\n",
       "      <th>Track_number_PV</th>\n",
       "      <th>Tracks_number</th>\n",
       "      <th>Tracks_number_passed</th>\n",
       "      <th>Vertex_chi2</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2461.369995</td>\n",
       "      <td>3958.329995</td>\n",
       "      <td>5521.529995</td>\n",
       "      <td>5553.419995</td>\n",
       "      <td>2351.519995</td>\n",
       "      <td>116.829995</td>\n",
       "      <td>1457.219995</td>\n",
       "      <td>3004.729995</td>\n",
       "      <td>3.267565</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>2.828167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3361.719998</td>\n",
       "      <td>3853.869998</td>\n",
       "      <td>4022.719998</td>\n",
       "      <td>4362.719998</td>\n",
       "      <td>936.834998</td>\n",
       "      <td>1.512528</td>\n",
       "      <td>259.576998</td>\n",
       "      <td>289.585998</td>\n",
       "      <td>3.461578</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.151478</td>\n",
       "      <td>2.828167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>790.218991</td>\n",
       "      <td>5305.539991</td>\n",
       "      <td>4305.589991</td>\n",
       "      <td>4309.419991</td>\n",
       "      <td>891.455991</td>\n",
       "      <td>190.981991</td>\n",
       "      <td>270.212991</td>\n",
       "      <td>252.402991</td>\n",
       "      <td>3.612741</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.179642</td>\n",
       "      <td>2.828167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2631.380000</td>\n",
       "      <td>11106.300000</td>\n",
       "      <td>3147.820000</td>\n",
       "      <td>3840.460000</td>\n",
       "      <td>419.138000</td>\n",
       "      <td>82.668400</td>\n",
       "      <td>243.689000</td>\n",
       "      <td>242.731000</td>\n",
       "      <td>3.459510</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.147980</td>\n",
       "      <td>2.828167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4343.369999</td>\n",
       "      <td>6410.599999</td>\n",
       "      <td>3811.409999</td>\n",
       "      <td>4777.559999</td>\n",
       "      <td>421.900999</td>\n",
       "      <td>6.995589</td>\n",
       "      <td>263.454999</td>\n",
       "      <td>266.133999</td>\n",
       "      <td>3.851009</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8.621519</td>\n",
       "      <td>2.828167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   EventID  Label         Mass  Corrected_mass           Pt       Pt_sum  \\\n",
       "0        0      1  2461.369995     3958.329995  5521.529995  5553.419995   \n",
       "1        1      1  3361.719998     3853.869998  4022.719998  4362.719998   \n",
       "2        1      1   790.218991     5305.539991  4305.589991  4309.419991   \n",
       "3        1      1  2631.380000    11106.300000  3147.820000  3840.460000   \n",
       "4        1      1  4343.369999     6410.599999  3811.409999  4777.559999   \n",
       "\n",
       "        Pt_min     IP_chi2  IP_chi2_sum  Flight_distance  Pseudorapidity  \\\n",
       "0  2351.519995  116.829995  1457.219995      3004.729995        3.267565   \n",
       "1   936.834998    1.512528   259.576998       289.585998        3.461578   \n",
       "2   891.455991  190.981991   270.212991       252.402991        3.612741   \n",
       "3   419.138000   82.668400   243.689000       242.731000        3.459510   \n",
       "4   421.900999    6.995589   263.454999       266.133999        3.851009   \n",
       "\n",
       "   Track_number_PV  Tracks_number  Tracks_number_passed  Vertex_chi2    Weight  \n",
       "0                0              2                     2     0.000095  2.828167  \n",
       "1                0              2                     1     1.151478  2.828167  \n",
       "2                0              2                     1     0.179642  2.828167  \n",
       "3                1              2                     1     2.147980  2.828167  \n",
       "4                1              3                     1     8.621519  2.828167  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training features\n",
    "\n",
    "Exclude `EventID`, `Label` and `Weight` from the features set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['IP_chi2_sum',\n",
       " 'Flight_distance',\n",
       " 'Pt',\n",
       " 'Tracks_number',\n",
       " 'Pt_sum',\n",
       " 'Corrected_mass',\n",
       " 'Track_number_PV',\n",
       " 'Pseudorapidity',\n",
       " 'Tracks_number_passed',\n",
       " 'Mass',\n",
       " 'IP_chi2',\n",
       " 'Pt_min',\n",
       " 'Vertex_chi2']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = list(set(data.columns) - {'EventID', 'Label', 'Weight'})\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide training data into 2 parts\n",
    "Here `train_test_split_group` function is used to divide into 2 parts to preserve secondary vertices from the same events in the same part of data (training or test). First argument should be events ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data, validation_data = train_test_split_group(data.EventID, data, random_state=11, train_size=0.66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple gradient boosting from `sklearn` training\n",
    "\n",
    "We take all secondary vertices (SVs) for all events and train on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
       "              max_depth=6, max_features=8, max_leaf_nodes=None,\n",
       "              min_samples_leaf=100, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=13, subsample=0.8, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb = GradientBoostingClassifier(learning_rate=0.1, n_estimators=100, subsample=0.8, random_state=13,\n",
    "                                min_samples_leaf=100, max_depth=6, max_features=8)\n",
    "gb.fit(training_data[features], training_data.Label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare predictions, labels and weights for events (not for SVs!) on the cross-validation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_mean(event_ids, values):\n",
    "    \"\"\" fore each event computes average of given values \"\"\"\n",
    "    number_of_sv_in_event = numpy.bincount(event_ids)\n",
    "    return numpy.bincount(event_ids, weights=values) / number_of_sv_in_event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.,   3.,  nan,   3.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of usage\n",
    "compute_mean(event_ids=[0, 1, 3, 1], values=[1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict each SV\n",
    "proba = gb.predict_proba(validation_data[features])\n",
    "events_ids = numpy.unique(validation_data.EventID)\n",
    "\n",
    "# compute number of SVs in each event\n",
    "number_of_sv_in_event = numpy.bincount(validation_data.EventID)\n",
    "\n",
    "# compute predictions for events (take the mean value of predictions for SVs forming an event)\n",
    "events_proba = compute_mean(validation_data.EventID, proba[:, 1])[events_ids]\n",
    "\n",
    "# compute weights for events \n",
    "events_weights = compute_mean(validation_data.EventID, validation_data.Weight)[events_ids]\n",
    "\n",
    "# compute labels for events \n",
    "events_labels = compute_mean(validation_data.EventID, validation_data.Label)[events_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC for events (with weights) on the cross validation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94717430152820903"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(events_labels, events_proba, sample_weight=events_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare submission to kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# predict each SV in test sample\n",
    "kaggle_proba = gb.predict_proba(test[features])\n",
    "\n",
    "kaggle_ids = numpy.unique(test.EventID)\n",
    "# compute predictions for events (take the mean value of predictions for SVs forming an event)\n",
    "kaggle_events_proba = compute_mean(test.EventID, kaggle_proba[:, 1])[kaggle_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='datasets/baseline.csv' target='_blank'>datasets/baseline.csv</a><br>"
      ],
      "text/plain": [
       "/Users/antares/Yandex.Disk.localized/projects/MLHEP/mlhep2016/kaggle_triggers/datasets/baseline.csv"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "def create_solution(ids, proba, filename='baseline.csv'):\n",
    "    \"\"\"saves predictions to file and provides a link for downloading \"\"\"\n",
    "    pandas.DataFrame({'EventID': ids, 'Label': proba}).to_csv('datasets/{}'.format(filename), index=False)\n",
    "    return FileLink('datasets/{}'.format(filename))\n",
    "    \n",
    "create_solution(kaggle_ids, kaggle_events_proba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
