{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist_kw = dict(bins=60, normed=True, alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stability. Confidence Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare ROC curve stability for simple Tree and for any ensemble method. Do they have different confidence intervals for ROC curves and AUC indeed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "data = pandas.read_csv('datasets/training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variables = list(set(data.columns) - {'id', 'min_ANNmuon', 'mass', 'signal', 'production'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to compute CL for ROC curve and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import clone\n",
    "def compute_CL(models, x_fpr, iterations=30):\n",
    "    result = {}\n",
    "    for name, model in models.items():\n",
    "        aucs = []\n",
    "        rocs = []\n",
    "        # repeat training 30 times on different training sample\n",
    "        for iterations in range(30):\n",
    "            # divide randomly into train - test samples\n",
    "            train_ind, test_ind = train_test_split(range(len(data)))\n",
    "            train_data = data.ix[train_ind, :]\n",
    "            test_data = data.ix[test_ind, :]\n",
    "            # training and computing fpr and tpr (use clone method to clone model)\n",
    "            ...\n",
    "            # linear interpolation for roc curve\n",
    "            rocs.append(numpy.interp(x_fpr, fpr, tpr))\n",
    "            aucs.append(roc_auc_score(test_data.signal.values, probs))\n",
    "        mean_roc = numpy.mean(rocs, axis=0)\n",
    "        std_roc = numpy.std(rocs, axis=0)\n",
    "        print name, numpy.mean(aucs), numpy.std(aucs)\n",
    "        result[name] = (mean_roc, std_roc)\n",
    "    return result\n",
    "        \n",
    "def plot_roc_CL(x_fpr, mean_roc, std_roc, r_xlim=(0, 1), r_ylim=(0, 1)):\n",
    "    figsize(10, 8)\n",
    "    plot(x_fpr, mean_roc, label='mean', color='r')\n",
    "    plot(x_fpr, mean_roc + std_roc, label='+', color='b')\n",
    "    plot(x_fpr, mean_roc - std_roc, label='-', color='g')\n",
    "    legend()\n",
    "    xlim(r_xlim[0], r_xlim[1])\n",
    "    ylim(r_ylim[0], r_ylim[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr_points = numpy.linspace(0, 1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence ROC curve intervals for simple tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "simple_tree = DecisionTreeClassifier(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence ROC curve intervals for simple tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compute results\n",
    "result = compute_CL({'tree': simple_tree, ...}, fpr_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC CL for tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_roc_CL(fpr_points, result['tree'][0], result['tree'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ROC CL for ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_roc_CL(fpr_points, result['GB'][0], result['GB'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatness models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare flatness models (uGBFL and knnAdaLoss), trained on the mass, with any ensemble model (1) trained on the mass and 2) without it).\n",
    "\n",
    "* Do they have comparable qualities? \n",
    "* What about CvM values? (check on `data_correlation.csv`)\n",
    "* Do you see non-flatness? (plot for several thresholds the local efficiency in the mass bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use**:\n",
    "    \n",
    "     from hep_ml.gradientboosting import UGradientBoostingClassifier\n",
    "     from hep_ml.losses import BinFlatnessLossFunction, \n",
    "\n",
    "**Note**:\n",
    "\n",
    "* `UGradientBoostingClassifier` has parameter `train_features`, in which `mass` (or another flatness vars) should be absent. It use `pandas.DataFrame` as input for fit.\n",
    "* Loss functions have `uniform_features` parameter, set it to `['mass']`, and uniform_label, for us it should be zero label (bck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pandas.read_csv('datasets/training.csv')\n",
    "data_correlation = pandas.read_csv('datasets/check_correlation.csv')\n",
    "train_features = list(set(data_correlation.columns) - {'id', 'signal', 'mass', 'SPDhits'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide train on train, test\n",
    "train_index, test_index = train_test_split(range(len(data)))\n",
    "train = data.iloc[train_index, :]\n",
    "test = data.iloc[test_index, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the order of non-correlated model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import compute_cvm\n",
    "compute_cvm(data_correlation.mass.values, numpy.random.random(size=len(data_correlation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define function to test model on cvm and calculate quality\n",
    "def test_model(model, features):\n",
    "    model_cvm = model.predict_proba(data_correlation[features])[:, 1]\n",
    "    model_corr = compute_cvm(data_correlation.mass.values, model_cvm)\n",
    "    print 'Correlation', model_corr\n",
    "    print 'AUC', roc_auc_score(test.signal.values, model.predict_proba(test[features])[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard model, trained on the mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Standard model, trained without mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FlatnessLoss with bins approximation for CvM computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hep_ml import gradientboosting, losses\n",
    "from hep_ml.gradientboosting import UGradientBoostingClassifier\n",
    "# define loss function, here fl_coefficient is flatness coefficient\n",
    "loss=losses.BinFlatnessLossFunction(uniform_features=['mass'], uniform_label=0, n_bins=20, fl_coefficient=5)\n",
    "ugb_flatness_loss = UGradientBoostingClassifier(loss=loss, train_features=train_features, \n",
    "                                                subsample=0.5, max_features=8, min_samples_leaf=50, max_depth=6)\n",
    "ugb_flatness_loss.fit(train, train.signal.values)\n",
    "test_model(ugb_flatness_loss, train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FlatnessLoss with knn approximation for CvM computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define loss function\n",
    "loss=losses.KnnFlatnessLossFunction(uniform_features=['mass'], uniform_label=0, n_neighbours=50, fl_coefficient=4)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knnAdaLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define loss function\n",
    "loss=gradientboosting.KnnAdaLossFunction(uniform_features=['mass'], uniform_label=0, knn=10)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare efficiencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_efficiencies(model, features, mass_bins=20):\n",
    "    figsize(8, 5)\n",
    "    probs = model.predict_proba(data_correlation[features])[:, 1]\n",
    "    m_b = data_correlation.mass.values\n",
    "    mass_p = numpy.percentile(m_b, numpy.linspace(0, 100, mass_bins + 1))\n",
    "    mass_centers = mass_p[:-1] + (mass_p[1:] - mass_p[:-1]) / 2. \n",
    "    bins_index = numpy.searchsorted(mass_p[1:-1], m_b)\n",
    "\n",
    "    for threshold in numpy.percentile(probs, [20, 40, 60, 80]):\n",
    "        eff_bins = numpy.bincount(bins_index, weights=(probs > threshold) * 1., minlength=len(mass_centers))\n",
    "        plot(mass_centers, eff_bins, label=threshold)\n",
    "    legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_efficiencies(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you combine several models to improve AUC \n",
    "\n",
    "Remember that the correlation shoud be less than 0.002 as in the kaggle competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
