{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Theano\n",
    "\n",
    "Theano is a python library and an optimizing compiler, which allows to define, optimizw and compute mathematical expressions effectively using multidimensional arrays.\n",
    "\n",
    "Features:\n",
    "\n",
    "* integration with NumPy\n",
    "* effective computation of gradient (can automatically build expressions to compute gradient)\n",
    "* fast and stable optimization (can recognize numerical inaccurate expressions and compute them using more stable algorithms)\n",
    "* clear using GPU\n",
    "* dynamic C++ code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import \n",
    "* theano\n",
    "* theano.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import theano.tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbolic expessions for tensors\n",
    "Theanoâ€™s strength is in expressing symbolic calculations involving tensors. There are many types of symbolic expressions for tensors:\n",
    "\n",
    "* scalar\n",
    "* vector\n",
    "* matrix\n",
    "* tensor\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create two vectors and one scaler\n",
    "x = T.vector() \n",
    "y = T.vector()\n",
    "alpha = T.scalar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute \n",
    "$x + \\alpha y + (\\sum x_i, ... ,\\sum x_i)^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define mathematical expression (you can use any function, which you use for NumPy arrays)\n",
    "z = x + alpha * y + T.sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile defined expression\n",
    "`theano.function`\n",
    "\n",
    "Returns a callable object that will calculate outputs from inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input variables, output expessions\n",
    "compiled_expr = theano.function([x, y, alpha], z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute compiled expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_val = numpy.arange(10)\n",
    "y_val = numpy.arange(10)\n",
    "alpha_val = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compiled_expr(x_val, y_val, alpha_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "* compute $(<x, \\alpha y> + <\\beta x, y>)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create two vectors and one scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Names for expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define \n",
    "x = T.vector(name='x') \n",
    "y = T.vector(name='y')\n",
    "alpha = T.scalar(name='a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z = alpha * x * T.log(y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `theano.printing.pprint()`\n",
    "Theano provides the functions `theano.printing.pprint()` and `theano.printing.debugprint()` to print a graph to the terminal before or after compilation. `pprint()` is more compact and math-like, `debugprint()` is more verbose. Theano also provides `pydotprint()` that creates an image of the function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.pprint(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.pprint(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compiled_expr = theano.function(inputs=[x, y, alpha], outputs=[z], name='function')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.printing.debugprint(compiled_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.printing.pydotprint(compiled_expr, outfile=\"graph.png\", var_with_name_simple=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`grad` returns symbolic gradients for one or more variables with respect to some cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1D "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = T.scalar(name='x') \n",
    "func = T.log(x) * T.sinh(x)\n",
    "func_prime = T.grad(func, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.pprint(func_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check yourself that this expession indeed gradient!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "func_prime_function = theano.function([x], func_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = numpy.linspace(0.1, 2, 100)\n",
    "plot([func_prime_function(point) for point in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.vector(name='x') \n",
    "func = T.sum(x * x)\n",
    "func_prime = T.grad(func, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theano.pprint(func_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "func_prime_function = theano.function([x], func_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = numpy.linspace(0.1, 2, 10)\n",
    "x2 = numpy.linspace(0.1, 2, 10)\n",
    "[func_prime_function(numpy.array([point1, point2])) for point1, point2 in zip(x1, x2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check yourself that this result is correct!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared variables\n",
    "`theano.shared` returns a SharedVariable variable, initialized with a copy or reference of `value`.\n",
    "\n",
    "Variable with Storage that is shared between functions that it appears in. These variables are meant to be created by registered shared constructors (see `shared_constructor()`).\n",
    "The user-friendly constructor is `shared()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = theano.shared(numpy.arange(10, dtype=float), name='weight')\n",
    "x = T.vector('x')\n",
    "func = theano.function([x], T.sum(x * w))\n",
    "func_grad = theano.function([x], T.grad(T.sum(x * w), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "func_grad(numpy.arange(10) * 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.vector('x')\n",
    "A = T.matrix('A')\n",
    "z = A.dot(x)\n",
    "normAx = theano.function([x, A], z.dot(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute this expression by yourself:\n",
    "\n",
    "$A = [[1, 1], [1, 1]]$\n",
    "\n",
    "$x = [0, 2]^T$\n",
    "\n",
    "What is the answer?\n",
    "\n",
    "Now compare it with theano result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "normAx([0, 2], [[1, 1], [1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_grad = theano.function([x, A], T.grad(z.dot(z), x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute gradient by yourself!!! Write its expression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(w, x) | grad(x) = w$\n",
    "\n",
    "$(Ax, Ax) | grad(x) = 2 (A^T A) x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute gradient for \n",
    "$A = [[0, 1], [1, 1]]$\n",
    "\n",
    "**in point**:\n",
    "$x = [1, 1]^T$\n",
    "\n",
    "What is the answer?\n",
    "\n",
    "Now compute the same with theano\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "norm_grad([0, 1], [[1, 1], [1, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression \n",
    "\n",
    "Write logistic regression algorithm using theano! Now this is really very simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import function to create toy dataset for classification\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from sklearn.cross_validation import train_test_split \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = 10 # number of features\n",
    "centers = 2 # number of classes\n",
    "X, y = make_blobs(n_samples=10000, centers=centers, n_features=n_features)\n",
    "X, X_test, y, y_test = train_test_split(X, y, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create theano objects for data(they should be shared because we know X, y)\n",
    "X_ = theano.shared(X, name='X')\n",
    "y_ = theano.shared(y, name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define vector of weights, we don't know it, that is why:\n",
    "w = T.vector(name='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic regression description:\n",
    "$p_i = sigmoid(\\sum_j X_{ij}w_j)$\n",
    "\n",
    "$loss=\\sum y_i \\log{p} + (1-y_i)\\log{(1 - p)}$\n",
    "\n",
    "$-loss \\to min$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write expression for probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write expression for loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# compile loss expression, compile gradient expression for loss\n",
    "loss_function = ...\n",
    "loss_grad = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have expessions for loss and it gradient and we need some method of optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# minimize loss function using its gradient\n",
    "result = minimize(fun=loss_function, x0=numpy.zeros(n_features), jac=loss_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w_optimal = result['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now predict output of logistic regression for the test sample and compute AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = X_test.dot(w_optimal)\n",
    "roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Is it simple with theano?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another implementation of logistic regression (add shift and l2 regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_steps = 1000\n",
    "X_ = T.matrix(\"x\")\n",
    "y_ = T.vector(\"y\")\n",
    "w = theano.shared(numpy.random.random(X.shape[1]), name=\"w\")\n",
    "# Add shift to the model\n",
    "b = theano.shared(0., name=\"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_sig = T.nnet.sigmoid(T.dot(X_, w) - b) \n",
    "llh_ = y_ * T.log(p_sig) + (1 - y_) * T.log(1 - p_sig)\n",
    "minus_llh_ = - llh_\n",
    "cost = minus_llh_.mean() + 0.01*(w**2).sum() # add l2 regularization with parameter 0.01 / The cost to optimize\n",
    "grad_w, grad_b = T.grad(cost, [w, b])\n",
    "# define updates after run the function\n",
    "train = theano.function(inputs=[X_, y_], outputs=[p_sig, minus_llh_], \n",
    "                        updates=[[w, w - 0.001*grad_w], [b, b - 0.001*grad_b]], name = \"train\")\n",
    "predict = theano.function(inputs=[X_], outputs=p_sig, name = \"predict\")\n",
    "\n",
    "for i in range(training_steps):\n",
    "    probs, err = train(X, y)\n",
    "pred = predict(X_test)\n",
    "roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-hidden layer\n",
    "This is a simple NN description with one hidden layer:\n",
    "\n",
    "Params: \n",
    "\n",
    "* W,\n",
    "\n",
    "* v\n",
    "\n",
    "Calculations:\n",
    "\n",
    "* h = sigmoid(X_.dot(W))\n",
    "* output = v.dot(h)\n",
    "* p_sig = sigmoid(output)\n",
    "* p_bck = 1 - p_sig\n",
    "\n",
    "\n",
    "Code it using theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using 1-hidden layer NN write 2-layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate another dataset\n",
    "X, y = make_moons(n_samples=20000)\n",
    "X, X_test, y, y_test = train_test_split(X, y, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scatter(X[:, 0], X[:, 1], c=y, alpha=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function fo NN to modify it by a simple way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_predict_NN(X, y, X_test, activate_functions, output_function, hidden_layers):\n",
    "    X_ = theano.shared(X, name='X')\n",
    "    y_ = theano.shared(y, name='y')\n",
    "    param = T.vector()\n",
    "    dim = X.shape[1]\n",
    "    \n",
    "    def activation(data_, parameter):\n",
    "        n_previous = 0\n",
    "        dim_previous = dim\n",
    "        h = data_\n",
    "        for n_hidden, func in zip(hidden_layers, activate_functions):\n",
    "            N = dim_previous * n_hidden\n",
    "            W_ = parameter[n_previous:n_previous + N].reshape((dim_previous, n_hidden))\n",
    "            h = func(h.dot(W_))\n",
    "            dim_previous = n_hidden\n",
    "            n_previous += N\n",
    "\n",
    "        # output     \n",
    "        v_ = parameter[n_previous:]\n",
    "        output = h.dot(v_)\n",
    "        n_previous = n_previous + dim_previous\n",
    "        \n",
    "        return output_function(output), n_previous\n",
    "\n",
    "    p_sig = activation(X_, param)[0]\n",
    "    p_bck = 1 - p_sig\n",
    "    llh_ = y_.dot(T.log(p_sig)) + (1 - y_).dot(T.log(p_bck))\n",
    "    minus_llh_ = -llh_\n",
    "    \n",
    "    # optimize\n",
    "    loss_function = theano.function([param], minus_llh_)\n",
    "    loss_grad = theano.function([param], theano.grad(minus_llh_, param))\n",
    "    result = minimize(loss_function, numpy.random.normal(size=activation(X_, param)[1]), jac=loss_grad)\n",
    "    optimal_params = result['x']\n",
    "    \n",
    "    # predict data\n",
    "    data = T.matrix()\n",
    "    compiled_activation = theano.function([data, param], activation(data, param)[0])\n",
    "    \n",
    "    return compiled_activation(X_test, optimal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate another dataset\n",
    "X, y = make_moons(n_samples=1000)\n",
    "# add noise to data\n",
    "X += numpy.random.random(size=X.shape[0] * X.shape[1]).reshape(X.shape)\n",
    "X, X_test, y, y_test = train_test_split(X, y, train_size=0.5)\n",
    "\n",
    "pred = fit_predict_NN(X, y, X_test, [T.nnet.sigmoid] * 3, T.nnet.sigmoid, [20, 10, 5])\n",
    "roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## activation intermediate functions:\n",
    "* sigmoid (which we used)\n",
    "* relu \n",
    "* softplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU - rectifier linear unit.\n",
    "\n",
    "In the context of artificial neural networks, the rectifier is an activation function defined as\n",
    "\n",
    "$f(x) = \\max(0, x)$\n",
    "\n",
    "where x is the input to a neuron. This activation function has been argued to be more biologically plausible than the widely used logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart, the hyperbolic tangent. The rectifier is the most popular activation function for deep neural networks.\n",
    "\n",
    "A unit employing the rectifier is also called a rectified linear unit (ReLU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Softplus\n",
    "A smooth approximation to the rectifier is the analytic function\n",
    "\n",
    "$f(x) = \\ln(1 + e^x)$\n",
    "\n",
    "which is called the softplus function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_relu(alpha):\n",
    "    def relu(x):\n",
    "        return T.switch(x > 0, x, alpha * x)\n",
    "    return relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate another dataset\n",
    "X, y = make_moons(n_samples=1000)\n",
    "X += numpy.random.random(size=X.shape[0] * X.shape[1]).reshape(X.shape)\n",
    "X, X_test, y, y_test = train_test_split(X, y, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = fit_predict_NN(X, y, X_test, [T.nnet.softplus] * 3, T.nnet.sigmoid, [10, 10, 5])\n",
    "roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = fit_predict_NN(X, y, X_test, [generate_relu(0.5)] * 2, T.nnet.sigmoid, [10, 10])\n",
    "roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclassification  Problem\n",
    "\n",
    "In this case we need use `softmax` activate fucntion for the last layer\n",
    "\n",
    "**TODO**:\n",
    "\n",
    "* Write multiclassification NN\n",
    "* add shift to the NN formula ( h = (x, w) + b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit_predict_mult_NN(X, y, X_test, activate_functions, hidden_layers):\n",
    "    X_ = theano.shared(X, name='X')\n",
    "    y_ = theano.shared(y, name='y')\n",
    "    param = T.vector()\n",
    "    n_class = len(numpy.unique(y))\n",
    "    dim = X.shape[1]\n",
    "    \n",
    "    def activation(data_, parameter):\n",
    "        n_previous = 0\n",
    "        dim_previous = dim\n",
    "        h = data_\n",
    "        for n_hidden, func in zip(hidden_layers, activate_functions):\n",
    "            N = dim_previous * n_hidden\n",
    "            W_ = parameter[n_previous:n_previous + N].reshape((dim_previous, n_hidden))\n",
    "            n_previous += N\n",
    "            b = parameter[n_previous: n_previous + n_hidden]\n",
    "            n_previous += n_hidden\n",
    "            h = func(h.dot(W_) + 0. * b[numpy.newaxis, :]) # here is brodcasing for b => it will be copied for each row\n",
    "            dim_previous = n_hidden\n",
    "\n",
    "        # output     \n",
    "        v_ = parameter[n_previous:].reshape((dim_previous, n_class))\n",
    "        output = h.dot(v_)\n",
    "        n_previous = n_previous + dim_previous * n_class\n",
    "        \n",
    "        return T.nnet.softmax(output), n_previous\n",
    "    \n",
    "    p_ = activation(X_, param)[0]\n",
    "    llh_ = T.log(p_[T.arange(len(X)), y_]).sum()\n",
    "    minus_llh_ = -llh_\n",
    "    \n",
    "    # optimize\n",
    "    loss_function = theano.function([param], minus_llh_)\n",
    "    loss_grad = theano.function([param], theano.grad(minus_llh_, param))\n",
    "    result = minimize(loss_function, numpy.random.normal(size=activation(X_, param)[1]), \n",
    "                      jac=loss_grad)\n",
    "    optimal_params = result['x']\n",
    "    \n",
    "    data = T.matrix()\n",
    "    compiled_activation = theano.function([data, param], activation(data, param)[0])\n",
    "\n",
    "    return compiled_activation(X_test, optimal_params)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_features = 20\n",
    "n_class = 5\n",
    "X, y = make_blobs(n_samples=2000, centers=n_class, n_features=n_features)\n",
    "X += (numpy.random.normal(size=X.shape[0]*X.shape[1]) * 10).reshape(X.shape)\n",
    "X, X_test, y, y_test = train_test_split(X, y, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "for func in [T.nnet.sigmoid, T.nnet.softmax, generate_relu(0.5)]:\n",
    "    pred = fit_predict_mult_NN(X, y, X_test, [], [])\n",
    "    imshow(confusion_matrix(y_test, numpy.argmax(pred, axis=1)), interpolation='nearest')\n",
    "    plt.xticks(range(n_class), range(n_class))\n",
    "    plt.yticks(range(n_class), range(n_class))\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net interafce in `hep_ml`: just write activation function using `theano`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import hep_ml\n",
    "from hep_ml import nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hep_ml.nnet import AbstractNeuralNetworkClassifier\n",
    "\n",
    "class SimpleNeuralNetwork(AbstractNeuralNetworkClassifier):\n",
    "    def prepare(self):\n",
    "        # getting number of layers in input, hidden, output layers\n",
    "        # note that we support only one hidden layer here\n",
    "        n1, n2, n3 = self.layers_\n",
    "        \n",
    "        # creating parameters of neural network\n",
    "        W1 = self._create_matrix_parameter('W1', n1, n2)\n",
    "        W2 = self._create_matrix_parameter('W2', n2, n3)\n",
    "        \n",
    "        # defining activation function\n",
    "        def activation(input):\n",
    "            first = T.nnet.sigmoid(T.dot(input, W1))\n",
    "            return T.dot(first, W2)\n",
    "\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=2000)\n",
    "X += numpy.random.random(size=X.shape[0] * X.shape[1]).reshape(X.shape)\n",
    "X, X_test, y, y_test = train_test_split(X, y, train_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnet_simple = SimpleNeuralNetwork()\n",
    "nnet_simple.fit(X, y)\n",
    "pred = nnet_simple.predict_proba(X_test)[:, 1]\n",
    "print roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite above binary classification NN into this interace \n",
    "\n",
    "Here is you daon't need to add `b` parameter, this interface does it and includes additional column in `X`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyNeuralNetwork(AbstractNeuralNetworkClassifier):\n",
    "    def prepare(self):\n",
    "        # getting number of layers in input, hidden, output layers\n",
    "        ...\n",
    "        \n",
    "        # defining activation function\n",
    "        def activation(input):\n",
    "            ...\n",
    "\n",
    "        return activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nnet_my = MyNeuralNetwork(layers=[10, 5, 5])\n",
    "nnet_my.fit(X, y)\n",
    "pred = nnet_my.predict_proba(X_test)[:, 1]\n",
    "print roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Own rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# your ideas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN in `hep_ml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hep_ml.nnet import PairwiseNeuralNetwork, RBFNeuralNetwork, MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for models in [PairwiseNeuralNetwork(), RBFNeuralNetwork(), MLPClassifier()]:\n",
    "    models.fit(X, y)\n",
    "    pred = models.predict_proba(X_test)[:, 1]\n",
    "    print roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging over NN - meta algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base = RBFNeuralNetwork()\n",
    "meta_ada = AdaBoostClassifier(base_estimator=base, n_estimators=10, learning_rate=0.05)\n",
    "meta_ada.fit(X, y)\n",
    "pred = meta_ada.predict_proba(X_test)[:, 1]\n",
    "print roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base = RBFNeuralNetwork()\n",
    "meta_bagging = BaggingClassifier(base_estimator=base, n_estimators=50, max_samples=0.7)\n",
    "meta_bagging.fit(X, y)\n",
    "pred = meta_bagging.predict_proba(X_test)[:, 1]\n",
    "print roc_auc_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oy! This can really work for simple data! Try apply this meta algorithms possibilities to your analysis and improve your models!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
